#!/usr/bin/python

from optparse import OptionParser


# Create option parser

req_doc = """


  module                The module from which maps are to be generated.
  database-file         The name of the database file produced by the mcmc.
  burn                  The number of initial MCMC iterations to discard. 
  pred-pts              A csv file containing the lon, lat, (time,) and 
                        associated covariate values where you want to predict.
                        
                        NOTE: time must be in units of DECIMAL YEARS SINCE
                        2009. That means it will usually be negative. That
                        is OK.

"""

p = OptionParser('usage: %prog module database-file burn pred-pts [options]' + req_doc)
p.add_option('-t','--thin',help='How much to thin the MCMC trace. Defaults to 1, meaning no thinning. This is recommended unless it takes too long.',dest='thin',type='int')
p.add_option('-i','--iter',help='The total number of samples to use in generating the map. Defaults to 20000',dest='total',type='int')

p.set_defaults(thin=50)
p.set_defaults(total=20000)


(o, args) = p.parse_args()
if len(args) != 4:
    raise ValueError, 'You must supply exactly four positional arguments. You supplied %i.'%len(args)

o.module, o.hf_name, o.burn, o.pred_input = args
o.burn = int(o.burn)



import matplotlib
matplotlib.use('PDF')
matplotlib.interactive(False)

from map_utils import *
from generic_mbg import *
import tables as tb
import numpy as np
import os, imp, sys, time

import pylab as pl
from pylab import csv2rec    


# Load up given module and load its relevant contents

mod_path, mod_name = os.path.split(o.module)
mod_basename, mod_ext = os.path.splitext(mod_name)
mod_search_path = [mod_path, os.getcwd()] + sys.path
mod = imp.load_module(mod_basename, *imp.find_module(mod_basename, mod_search_path))

for n in ['f_name', 'nugget_name', 'f_has_nugget', 'postproc','x_name']:
    try:
        exec("%s=getattr(mod,'%s')"%(n,n))
    except:
        cls,inst,tb = sys.exc_info()
        new_inst = cls('Could not import %s from %s. Tell Anand. Original error message:\n\n\t%s'%(n,mod_name,inst.message))
        raise cls,new_inst,tb


# Parse input file

input = csv2rec(file(o.pred_input,'U'))
lon = maybe_convert(input, 'lon', 'float')
lat = maybe_convert(input, 'lat', 'float')
if hasattr(input, 't'):
    t = maybe_convert(input, 't', 'float')
    x = combine_st_inputs(lon,lat,t)
else:
    x = combine_spatial_inputs(lon,lat)
pos = maybe_convert(input, 'pos', 'float')
neg = maybe_convert(input, 'neg', 'float')
n_sampled = pos+neg
if np.any(n_sampled==0):
    where_zero = np.where(n_sampled==0)[0]
    raise ValueError, 'Pos+neg = 0 in the rows (starting from zero):\n %s'%where_zero

non_cov_columns = {}
if hasattr(mod, 'non_cov_columns'):
    non_cov_coltypes = mod.non_cov_columns
else:
    non_cov_coltypes = {}
non_cov_colnames = non_cov_coltypes.keys()

covariate_dict = {}
for n in input.dtype.names:
    if n not in ['lon','lat','pos','neg']:
        if n in non_cov_colnames:
            non_cov_columns[n] = maybe_convert(input, n, non_cov_coltypes[n])
        else:
            covariate_dict[n]=maybe_convert(input, n, 'float')


# Create predictive locations and open hdf5 archive

hf = tb.openFile(o.hf_name)


# Create predictive samples

t1 = time.time()
p_samples = hdf5_to_samps(hf,x,o.burn,o.thin,o.total,[sample_reduce],f_name,f_has_nugget,x_name,covariate_dict,nugget_name,postproc,sample_finalize,**non_cov_columns).T
n_samples = np.array([pm.rbinomial(n_sampled[i], p_samples[:,i]) for i in xrange(len(n_sampled))])
print '\nPredictive samples drawn in %f seconds\n'%(time.time() - t1)


# Write out

hf_path, hf_basename  = os.path.split(o.hf_name)
base_outname = os.path.splitext(hf_basename)[0]
val_dir = os.path.join(hf_path, base_outname+'-validation')
try:
    os.mkdir(val_dir)
except OSError:
    pass
os.chdir(val_dir)

# Mean errors
errors = np.rec.fromarrays([np.empty(len(input)) for i in (0,1,2)], names='mean error,mean abs error,RMS error')
for i in xrange(len(input)):

    p_obs = pos[i]/float(neg[i])
    dev = n_samples[i]/float(n_sampled[i]) - p_obs

    pl.clf()
    pdf, bins, patches = pl.hist(p_samples[i], 50, normed=True,facecolor='.3')
    yext = pdf.max()
    pl.plot([p_obs,p_obs],[0,yext],'k-.',label='Observed frequency')
    pl.legend(loc=0)
    pl.xlabel('Frequency')
    pl.ylabel('Predictive density')
    pl.savefig('%i.pdf'%i)    
    
    errors['mean error'][i] = np.mean(dev)
    errors['mean abs error'][i] = np.mean(np.abs(dev))
    errors['RMS error'][i] = np.sqrt(np.mean(dev**2))

for k,v in [('p_samples',p_samples),('n_samples',n_samples)]:
    tb.openFile(k,'w').createCArray('/','samples',
                        tb.Float32Atom(), 
                        shape = v.shape, 
                        filters=tb.Filters(complevel=1))[:] = v[:]
    
pl.rec2csv(errors,'mean-errors.csv')

for f in validation_fns:
    print 'Generating plot of %s'%f.__name__
    pl.clf()
    f(p_samples.T, n_samples.T, pos, neg)
    pl.savefig(f.__name__ + '.pdf', transparent=True)